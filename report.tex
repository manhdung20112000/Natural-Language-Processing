\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{enumitem}
\graphicspath{ {.img/stem/} }

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Document Classification: 20 Newsgroup
  \vspace{.35cm} \\
  \Large{\normalfont INT3406 21 - Group 3} 
}

\author{
  \large Group 3
  \vspace{.1cm} \\
  Pham Truong Giang - 1802xxxx\\
  Nguyen Manh Dung - 18020370\\
  Nguyen Phuc Hai - 1802xxxx\\
  Le Bang Giang - 1802xxxx\\
}

\begin{document}

\maketitle

\begin{abstract}
  Your abstract should motivate the problem, describe your goals, 
  and highlight your main findings. Given that your project is still in progress, it is okay if your findings are what you are still working on.
\end{abstract}


\section{Introduction}

\begin{enumerate}[label=1.\arabic*]
    \item abstract
    \item abstract
\end{enumerate}


\section{Preprocessing}
Data preprocessing is an essential step in building Machine Learning models. 
In natural language processing (NLP), text preprocessing can simply be understood as the process to transform raw text data into a form that is \emph{\textbf{predictable}} and \emph{\textbf{analyzable}}.\\
However, the preprocess steps depend mostly on the task. One task’s ideally preprocessing can become another task “nightmare”. 
So it’s important to keep in mind that preprocessing is not a one-size-fits-all approach.
\begin{enumerate}[label=2.\arabic*]
    \item Lowercasing \\
    The simplest technique to start preprocessing data is lowercasing ALL the text. Although simple as it is, lowercasing is the most effective form of text preprocessing that can be applicable to most NLP problems. \\
    It’s easy to confuse the model that “Vietnam” and “vietnaM” are 2 different words. Although it has the same meaning, refer to the same country. Here is the example of how lowercasing solves the issue.    
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline 
            \textbf{Raw} & \textbf{Lowercasing} \\
            \hline
            Vietnam & \\
            vietNam & vietnam\\
            VIETNAM & \\
            vietnaM & \\
            \hline
            Autumn & \\
            autumn & autumn\\ 
            AuTuMn & \\ 
            autumn & \\
            \hline
        \end{tabular}
    \end{center}

    \item Map different word to canonical form \\
    Languages we speak and write are made up of several words often derived from one another.
    When a language contains words that are derived from another word as their use in the speech changes is called \emph{\textbf{Inflected Language}}.\\
    For simple, we can simply understand that an inflected word will have a \emph{common root}. 

    \begin{center}
        \begin{tabular}{|c|c|}
            \hline 
            \textbf{Inflected} & \textbf{Root} \\
            \hline
            playing & \\
            played & play\\
            player & \\
            \hline
            better & \\
            best & good\\ 
            good & \\ 
            \hline
        \end{tabular}
    \end{center}

    \begin{enumerate}[label=2.2.\arabic*]
        \item Stemming \\
        Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. \\
        Stemming uses a crude heuristic process that chops off the ends of words in the hope of correctly transforming words into its root form.

        
        \item Lemmatization \\
    \end{enumerate}
    Stemming and Lemmatization are widely used in \emph{tagging systems, indexing, SEOs, Web search results, and information retrieval}. 
    For example, searching for fish on Google will also result in fishes, fishing as fish is the stem of both words. \\

    \item Stop word \\
    \item Noise removal \\
    \item Summary \\
\end{enumerate}



\section{Feature Extraction}

\begin{enumerate}[label=3.\arabic*]
    \item Bag of Word (BoW) \\
    \begin{enumerate}[label=3.1.\arabic*]
        \item Brief Explaination
        \item Algorithm
        \item Implementation
        \item Discussion
    \end{enumerate}
    
    \item Term Frequency – Inverse Document Frequency (TF – IDF)
    \begin{enumerate}[label=3.2.\arabic*]
        \item Brief Explaination
        \item Algorithm
        \item Implementation
        \item Discussion
    \end{enumerate}
    
    \item Word Embedding
    \begin{enumerate}[label=3.3.\arabic*]
        \item Brief Explaination
        \item Algorithm
        \item Implementation
        \item Discussion
    \end{enumerate}
\end{enumerate}


\section{Classification}

\begin{enumerate}[label=4.\arabic*]
    \item Linear Model
    \begin{enumerate}[label=4.1.\arabic*]
        \item Naive Bayes
        \item Logistic Regression (LR)
        \item Ridge Classification
        \item Perceptron
        \item Passive-Aggressive
    \end{enumerate}
    
    \item Non-parametric
    \begin{enumerate}[label=4.2.\arabic*]
        \item K-nearest neighbor (KNN)
        \item Support Vector Machine (SVM)
        \item Linear Support Vector Machine (LinearSVC)
    \end{enumerate}
    
    \item Tree-based Classifiers
    \begin{enumerate}[label=4.3.\arabic*]
        \item K-nearest neighbor (KNN)
        \item Support Vector Machine (SVM)
        \item Linear Support Vector Machine (LinearSVC)
    \end{enumerate}

    \item Graphical Classification
    \begin{enumerate}[label=4.4.\arabic*]
        \item Conditional Random Fields (CRFs)
        \item ...
    \end{enumerate}

    \item Neural Network
\end{enumerate}


\section{Summary}
Abstract


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
