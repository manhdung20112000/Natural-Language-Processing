\documentclass{article}

\usepackage[final]{neurips_2019}
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{amsfonts}
\usepackage{nicefrac}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{lipsum}
\usepackage{enumitem}
\graphicspath{ {.img/stem/} }

\newcommand{\note}[1]{\textcolor{blue}{{#1}}}

\title{
  Document Classification: 20 Newsgroup
  \vspace{.35cm} \\
  \Large{\normalfont INT3406 21 - Group 3} 
}

\author{
  \large Group 3
  \vspace{.1cm} \\
  Pham Truong Giang - 1802xxxx\\
  Nguyen Manh Dung - 18020370\\
  Nguyen Phuc Hai - 1802xxxx\\
  Le Bang Giang - 1802xxxx\\
}

\begin{document}

\maketitle

\begin{abstract}
  Your abstract should motivate the problem, describe your goals, 
  and highlight your main findings. Given that your project is still in progress, it is okay if your findings are what you are still working on.
\end{abstract}


\section{Introduction}

\begin{enumerate}[label=1.\arabic*]
    \item abstract
    \item abstract
\end{enumerate}


\section{Preprocessing}
Data preprocessing is an essential step in building Machine Learning models. 
In natural language processing (NLP), text preprocessing can simply be understood as the process to transform raw text data into a form that is \emph{\textbf{predictable}} and \emph{\textbf{analyzable}}.\\
However, the preprocess steps depend mostly on the task. One task’s ideally preprocessing can become another task “nightmare”. 
So it’s important to keep in mind that preprocessing is not a one-size-fits-all approach.
\begin{enumerate}[label=2.\arabic*]
    \item Lowercasing \\
    The simplest technique to start preprocessing data is lowercasing ALL the text. Although simple as it is, lowercasing is the most effective form of text preprocessing that can be applicable to most NLP problems. \\
    It’s easy to confuse the model that “Vietnam” and “vietnaM” are 2 different words. Although it has the same meaning, refer to the same country. Here is the example of how lowercasing solves the issue.    
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline 
            \textbf{Raw} & \textbf{Lowercasing} \\
            \hline
            Vietnam & \\
            vietNam & vietnam\\
            VIETNAM & \\
            vietnaM & \\
            \hline
            Autumn & \\
            autumn & autumn\\ 
            AuTuMn & \\ 
            autumn & \\
            \hline
        \end{tabular}
    \end{center}

    \item Map different word to canonical form \\
    Languages we speak and write are made up of several words often derived from one another.
    When a language contains words that are derived from another word as their use in the speech changes is called \emph{\textbf{Inflected Language}}.\\
    For simple, we can simply understand that an inflected word will have a \emph{common root}. 

    \begin{center}
        \begin{tabular}{|c|c|}
            \hline 
            \textbf{Inflected} & \textbf{Root} \\
            \hline
            playing & \\
            played & play\\
            player & \\
            \hline
            better & \\
            best & good\\ 
            good & \\ 
            \hline
        \end{tabular}
    \end{center}

    \begin{enumerate}[label=2.2.\arabic*]
        \item Stemming \\
        Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. \\
        Stemming uses a crude heuristic process that chops off the ends of words in the hope of correctly transforming words into its root form. \\
        
        \begin{center}
            \textbf{<<image>>}
        \end{center}
        
        \item Lemmatization \\
        Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization’s root word is called \emph{Lemma}. 
        A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\\
        \begin{center}
            \textbf{<<image>>}
        \end{center}
        Lemmatization and stemming seem to return different results from a human point of view. 
        However, research has proved that lemmatization provides no significant benefit than stemming does.
        
    \end{enumerate}
    Stemming and Lemmatization are widely used in \emph{tagging systems, indexing, SEOs, Web search results, and information retrieval}. 
    For example, searching for fish on Google will also result in fishes, fishing as fish is the stem of both words. \\
    
    \item Stop word \\
    Stop Words are words which do not contain important significance to be used in \emph{Search Queries}. 
    Usually, these words are filtered out from search queries because they return a vast amount of \textbf{\emph{unnecessary information}}. \\
    Mostly they are words that are commonly used in the English language such as 'as, the, be, are' etc.\\
    
    \begin{center}
        \textbf{<<image>>}
    \end{center}

    \textbf{\emph{nltk}} provides a list of english stop words that can be used directly in preprocessing. 
    However, depending on the situation we can append more words that have unnecessary information into the stop words list,
     later in this section will talk about this. \\
    
    \item Noise removal \\
    Noise removal is about removing characters digits and pieces of text that can interfere with your text analysis. Noise removal is one of the most essential text preprocessing steps. \\
    Noise need to be process before start stemming or lemmatization because it can lead to unable to recognize word in function, let’s look at this example:\\

    \begin{center}
        \textbf{<<image>>}
    \end{center}

    Noise can be all special characters that were used to format or characterize the data. In this case, it’s html hashtag, punctuation, special character, ... \\
    The main purpose of noise removal function is to clean all the surrounding noise to return the main data. With some cleaning, the result can stem as normal:\\

    \begin{center}
        \textbf{<<image>>}
    \end{center}

    \emph{In our assignment, we have 4 function to remove noise from the data, there is:}
    \begin{itemize}
        \item Removing html tag
        \item Removing url
        \item Removing special characters
        \item Removing word that length below 2 letters
    \end{itemize}


    \item Summary \\
\end{enumerate}



\section{Feature Extraction}

\begin{enumerate}[label=3.\arabic*]
    \item Bag of Word (BoW) \\
    \begin{enumerate}[label=3.1.\arabic*]
        \item Brief Explaination
        \item Algorithm
        \item Implementation
        \item Discussion
    \end{enumerate}
    
    \item Term Frequency – Inverse Document Frequency (TF – IDF)
    \begin{enumerate}[label=3.2.\arabic*]
        \item Brief Explaination
        \item Algorithm
        \item Implementation
        \item Discussion
    \end{enumerate}
    
    \item Word Embedding
    \begin{enumerate}[label=3.3.\arabic*]
        \item Brief Explaination
        \item Algorithm
        \item Implementation
        \item Discussion
    \end{enumerate}
\end{enumerate}


\section{Classification}

\begin{enumerate}[label=4.\arabic*]
    \item Linear Model
    \begin{enumerate}[label=4.1.\arabic*]
        \item Naive Bayes
        \item Logistic Regression (LR)
        \item Ridge Classification
        \item Perceptron
        \item Passive-Aggressive
    \end{enumerate}
    
    \item Non-parametric
    \begin{enumerate}[label=4.2.\arabic*]
        \item K-nearest neighbor (KNN)
        \item Support Vector Machine (SVM)
        \item Linear Support Vector Machine (LinearSVC)
    \end{enumerate}
    
    \item Tree-based Classifiers
    \begin{enumerate}[label=4.3.\arabic*]
        \item K-nearest neighbor (KNN)
        \item Support Vector Machine (SVM)
        \item Linear Support Vector Machine (LinearSVC)
    \end{enumerate}

    \item Graphical Classification
    \begin{enumerate}[label=4.4.\arabic*]
        \item Conditional Random Fields (CRFs)
        \item ...
    \end{enumerate}

    \item Neural Network
\end{enumerate}


\section{Summary}
Abstract


\bibliographystyle{unsrt}
\bibliography{references}

\end{document}
