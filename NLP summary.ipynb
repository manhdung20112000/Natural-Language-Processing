{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and visualize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.utils\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_train = fetch_20newsgroups(subset='train', shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(twenty_train.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twenty_train.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before extract data into vector using `Bag of Words`, `TF`, ... We need to clean the text data and this process to prepare (or clean) text data before encoding is called **text preprocessing**.\n",
    "\n",
    "***There are 3 main components:***\n",
    "- Tokenization\n",
    "- Normalization\n",
    "- Noise removal\n",
    "\n",
    "Paragraphs can be tokenized into sentences and sentences can be tokenized into words, it's **Tokenization**. **Normalization** aims to put all text on a level playing field, e.g., converting all characters to lowercase. **Noise removal** cleans up the text, e.g., remove extra whitespaces.\n",
    "\n",
    "***Text Preprocessing steps:*** \n",
    "- Remove HTML tags\n",
    "- Remove extra whitespaces\n",
    "- Convert accented characters to ASCII characters\n",
    "- Expand contractions\n",
    "- Remove special characters\n",
    "- Lowercase all texts\n",
    "- Convert number words to numeric form\n",
    "- Remove numbers\n",
    "- Remove stopwords\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manhd\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing URL\n",
    "def clean_url (text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#removing special characters\n",
    "def clean_special_character (text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', text)\n",
    "\n",
    "#removing upper case characters\n",
    "def clean_uppercase (text):\n",
    "    return str(text).lower()\n",
    "\n",
    "#tokenization\n",
    "def tokenization (text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "#removing stop words\n",
    "def clean_stop_word (tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "#steamming\n",
    "def steam (tokens):\n",
    "    return [PorterStemmer().stem(token) for token in tokens]\n",
    "\n",
    "#lenmatization\n",
    "def lenmatization (tokens):\n",
    "    return [WordNetLemmatizer().lemmatize(word=token, pos='v') for token in tokens]\n",
    "\n",
    "#remove the words having length <= 2\n",
    "def clean_length (tokens):\n",
    "    return [token for token in tokens if len(token) > 2]\n",
    "\n",
    "#convert back to string\n",
    "def convert_2_string (text):\n",
    "    return ' '.join(text)\n",
    "\n",
    "#apply all cleaner\n",
    "def clean (text):\n",
    "    res = clean_url(text)\n",
    "    res = clean_special_character(res)\n",
    "    res = clean_uppercase(res)\n",
    "    res = tokenization(res)\n",
    "    res = clean_stop_word(res)\n",
    "    res = lenmatization(res)\n",
    "    res = clean_length(res)\n",
    "    return convert_2_string(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " lerxst wam umd edu thing subject car nntp post host rac wam umd edu organization university maryland college park line wonder anyone could enlighten car saw day door sport car look late early call bricklin doors really small addition front bumper separate rest body know anyone tellme model name engine specs years production car make history whatever info funky look car please mail thank bring neighborhood lerxst\n"
     ]
    }
   ],
   "source": [
    "#example\n",
    "example = twenty_train.data[0]\n",
    "after_clean = clean(example)\n",
    "print(example, after_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Extraction** is used to transform each text into a numerical representation in the form of a vector. (This process can contain *Tokenization, Vectorization, etc*)\n",
    "\n",
    "Feature Extraction aims to reduce the number of features in a dataset by creating new features from the existing ones (and then discarding the original features)\n",
    "\n",
    "***Feature Extraction advantages:***\n",
    "- Accuracy improvements.\n",
    "- Overfitting risk reduction.\n",
    "- Speed up in training.\n",
    "- Improved Data Visualization.\n",
    "- Increase in explainability of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE using Bag of Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<4x9 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 19 stored elements in Compressed Sparse Row format>\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "pprint(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['h1', 'this', 'https', 'google', 'com', 'is', 'text', 'document', 'to', 'analyze', 'h1']\n"
     ]
    }
   ],
   "source": [
    "analyze = vectorizer.build_analyzer()\n",
    "print(analyze(\"<h1>This &*$) https://google.com is a text document to analyze.</h1>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
